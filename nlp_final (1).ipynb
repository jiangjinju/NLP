{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#give credit to Theo Viel from https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing\n",
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "glove = './input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "paragram =  './input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "wiki_news = './input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>853896</th>\n",
       "      <td>a74d7ae7bbfd1da83da0</td>\n",
       "      <td>What is a pilot test in research?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369167</th>\n",
       "      <td>48625812ea248aa7635a</td>\n",
       "      <td>How long do you have before a Snapchat streak ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942438</th>\n",
       "      <td>b8b1c5c92c104616796f</td>\n",
       "      <td>What are some tricks for mixing and mastering ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226864</th>\n",
       "      <td>f071f84dacb57b43b4b3</td>\n",
       "      <td>What would be the cost and estimated time of c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161391</th>\n",
       "      <td>e38fd4f4ee46cad5e565</td>\n",
       "      <td>Has being on Quora changed your conception of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218571</th>\n",
       "      <td>eed4396f2e4bcfde0bde</td>\n",
       "      <td>What is Skandar Keynes known for?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25324</th>\n",
       "      <td>04f85059238fb6ea61c1</td>\n",
       "      <td>Is water really wet? Or is the internet losing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182869</th>\n",
       "      <td>23bf13a2a78dfaca969a</td>\n",
       "      <td>Which are the Mark awarding faculties for IP i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290597</th>\n",
       "      <td>38e96e18fddaa4825098</td>\n",
       "      <td>If the communist China decides to invade Taiwa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529681</th>\n",
       "      <td>67b5a82082f28cd3993e</td>\n",
       "      <td>Where would Minato or go or what would happen ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "853896   a74d7ae7bbfd1da83da0   \n",
       "369167   48625812ea248aa7635a   \n",
       "942438   b8b1c5c92c104616796f   \n",
       "1226864  f071f84dacb57b43b4b3   \n",
       "1161391  e38fd4f4ee46cad5e565   \n",
       "1218571  eed4396f2e4bcfde0bde   \n",
       "25324    04f85059238fb6ea61c1   \n",
       "182869   23bf13a2a78dfaca969a   \n",
       "290597   38e96e18fddaa4825098   \n",
       "529681   67b5a82082f28cd3993e   \n",
       "\n",
       "                                             question_text  target  \n",
       "853896                   What is a pilot test in research?       0  \n",
       "369167   How long do you have before a Snapchat streak ...       0  \n",
       "942438   What are some tricks for mixing and mastering ...       0  \n",
       "1226864  What would be the cost and estimated time of c...       0  \n",
       "1161391  Has being on Quora changed your conception of ...       0  \n",
       "1218571                  What is Skandar Keynes known for?       0  \n",
       "25324    Is water really wet? Or is the internet losing...       0  \n",
       "182869   Which are the Mark awarding faculties for IP i...       0  \n",
       "290597   If the communist China decides to invade Taiwa...       0  \n",
       "529681   Where would Minato or go or what would happen ...       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50830</th>\n",
       "      <td>e6803496deb741f271d4</td>\n",
       "      <td>What's the best YouTube video to watch to lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39548</th>\n",
       "      <td>b2b26392aab2c2ee3d53</td>\n",
       "      <td>Who's the best mathematics teacher for general...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36861</th>\n",
       "      <td>a6320268cd11be86b9e4</td>\n",
       "      <td>Do you believe in brainwashing, and if so is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24870</th>\n",
       "      <td>70617568a7f635de7ab4</td>\n",
       "      <td>Which chapters contribute the most in ICSE bio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31829</th>\n",
       "      <td>8f950a9fa5b2a7de6b2a</td>\n",
       "      <td>Why has Quora sacrificed its valued intellectu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47800</th>\n",
       "      <td>d8586ab323d07be72d7e</td>\n",
       "      <td>How should I prepare for CGL 2018?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47181</th>\n",
       "      <td>d57bd0e7d95288dae9ce</td>\n",
       "      <td>What is the strongest spell that Zatana got?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25539</th>\n",
       "      <td>7361386ab4bf8f162d57</td>\n",
       "      <td>What does it mean on Spokeo when it says sourc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33192</th>\n",
       "      <td>95bc3ef0726aabc118ee</td>\n",
       "      <td>Can anyone become travel show host at the age ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17105</th>\n",
       "      <td>4d555457c9faf3663921</td>\n",
       "      <td>How many Bangladeshi lives in Germany?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        qid                                      question_text\n",
       "50830  e6803496deb741f271d4  What's the best YouTube video to watch to lear...\n",
       "39548  b2b26392aab2c2ee3d53  Who's the best mathematics teacher for general...\n",
       "36861  a6320268cd11be86b9e4  Do you believe in brainwashing, and if so is t...\n",
       "24870  70617568a7f635de7ab4  Which chapters contribute the most in ICSE bio...\n",
       "31829  8f950a9fa5b2a7de6b2a  Why has Quora sacrificed its valued intellectu...\n",
       "47800  d8586ab323d07be72d7e                 How should I prepare for CGL 2018?\n",
       "47181  d57bd0e7d95288dae9ce       What is the strongest spell that Zatana got?\n",
       "25539  7361386ab4bf8f162d57  What does it mean on Spokeo when it says sourc...\n",
       "33192  95bc3ef0726aabc118ee  Can anyone become travel show host at the age ...\n",
       "17105  4d555457c9faf3663921             How many Bangladeshi lives in Germany?"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "2d66b0bb5d54107db72a369ae647e741d4c5e47a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    if file == './input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "842717dfa3c059ca85af0d647a9d2106d6d7846f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "5410b83979e99bdfea893e1247262161983388aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "35ff13e167a299e8e609f5c7c66f67d53fa7a234",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "73c002db44fe882610369c9f4117ac01666c3fd3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "cdc651957eef872c639ad3a503cadfa1ba58ff59",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "0c5720dabf18bc493cab52372760062a2006aaf7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "856d886819b5ae0968a37deaed261db212431e2d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "fa81e3ae94e10e4a617c5506b6f78a29c616375d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "b13ead97ac1da73deb808dfdb342d3751dcafff9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mydatacleanning(df):\n",
    "    df['lowered_question'] = df['question_text'].apply(lambda x: x.lower())\n",
    "    df['treated_question'] = df['lowered_question'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "    df['treated_question'] = df['treated_question'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "    vocab = build_vocab(df['treated_question'])\n",
    "    print(\"Glove : \")\n",
    "    embed_glove = load_embed(glove)\n",
    "    oov_glove = check_coverage(vocab, embed_glove)\n",
    "    print(\"Paragram : \")\n",
    "    embed_paragram = load_embed(paragram)\n",
    "    oov_paragram = check_coverage(vocab, embed_paragram)\n",
    "    print(\"FastText : \")\n",
    "    embed_fasttext = load_embed(wiki_news)\n",
    "    oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "ceb5e309c24f28325e4afd52860e7ab70d6cd022",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df_original = pd.read_csv(\"../input/train.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "4096adc1280b2dc3bae2b7b64576cb2c2c1ce22b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 63.34% of vocab\n",
      "Found embeddings for  99.38% of all text\n",
      "Paragram : \n",
      "Found embeddings for 73.99% of vocab\n",
      "Found embeddings for  99.63% of all text\n",
      "FastText : \n",
      "Found embeddings for 48.17% of vocab\n",
      "Found embeddings for  98.80% of all text\n"
     ]
    }
   ],
   "source": [
    "train_df_clean=mydatacleanning(train_df_original)\n",
    "train_df, val_df = train_test_split(train_df_clean, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "6b3da28f643df786b092aa1ff9803a29c98ef848"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>lowered_question</th>\n",
       "      <th>treated_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834855</th>\n",
       "      <td>a399760b6ff8d91d45ee</td>\n",
       "      <td>Has anyone read Talon of God by Wesley Snipes ...</td>\n",
       "      <td>0</td>\n",
       "      <td>has anyone read talon of god by wesley snipes ...</td>\n",
       "      <td>has anyone read talon of god by wesley snipes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332052</th>\n",
       "      <td>4117d020b2cfc0f3bdfe</td>\n",
       "      <td>Are there any browsers with dual-engine WebKit...</td>\n",
       "      <td>0</td>\n",
       "      <td>are there any browsers with dual-engine webkit...</td>\n",
       "      <td>are there any browsers with dual  -  engine we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid                        ...                                                           treated_question\n",
       "834855  a399760b6ff8d91d45ee                        ...                          has anyone read talon of god by wesley snipes ...\n",
       "332052  4117d020b2cfc0f3bdfe                        ...                          are there any browsers with dual  -  engine we...\n",
       "\n",
       "[2 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(2)\n",
    "val_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "07f1874962d8ea0d4ec4c116325eed9ca9c64c95",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "460655ad0c4ab43368f0f60c3ee659055a071868"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [03:15, 11245.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# embdedding setup\n",
    "# Source https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "# Give credit to Miha Skalic from https://www.kaggle.com/mihaskalic/lstm-is-all-you-need-well-maybe-embeddings-also\n",
    "embeddings_index = {}\n",
    "f = open('./input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "abd816ab9eb357cb56795ca03744ec3735c6f705"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 16637.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert values to embeddings\n",
    "def text_to_array(text):\n",
    "    empyt_emb = np.zeros(300)\n",
    "    text = text[:-1].split()[:20]\n",
    "    embeds = [embeddings_index.get(x, empyt_emb) for x in text]\n",
    "    embeds+= [empyt_emb] * (20 - len(embeds))\n",
    "    return np.array(embeds)\n",
    "\n",
    "val_vects = np.array([text_to_array(X_text) for X_text in tqdm(val_df[\"treated_question\"][:3000])])\n",
    "val_y = np.array(val_df[\"target\"][:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "0fba5ab55200d7919845db015cbed7e22b44d101",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data providers\n",
    "batch_size = 64\n",
    "\n",
    "def batch_gen(train_df):\n",
    "    n_batches = math.ceil(len(train_df) / batch_size)\n",
    "    while True: \n",
    "        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
    "        for i in range(n_batches):\n",
    "            texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "            text_arr = np.array([text_to_array(text) for text in texts])\n",
    "            yield text_arr, np.array(train_df[\"target\"][i*batch_size:(i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "435d93f977625401b260d883b8ef5033d7eaec66",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "8bb115f0fc9df2660a124fa7eb9627a820d0ae36",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True),\n",
    "                        input_shape=(20, 300)))\n",
    "model.add(Bidirectional(CuDNNLSTM(64)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "73923dbe4cc97b1492d98b0c5e640be0bd9d4d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.1392 - acc: 0.9474 - val_loss: 0.1452 - val_acc: 0.9383\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1235 - acc: 0.9514 - val_loss: 0.1355 - val_acc: 0.9457\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1203 - acc: 0.9530 - val_loss: 0.1358 - val_acc: 0.9467\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1203 - acc: 0.9543 - val_loss: 0.1318 - val_acc: 0.9487\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1160 - acc: 0.9546 - val_loss: 0.1335 - val_acc: 0.9500\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1121 - acc: 0.9562 - val_loss: 0.1274 - val_acc: 0.9550\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1119 - acc: 0.9550 - val_loss: 0.1261 - val_acc: 0.9537\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1166 - acc: 0.9539 - val_loss: 0.1344 - val_acc: 0.9507\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1111 - acc: 0.9564 - val_loss: 0.1260 - val_acc: 0.9513\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1107 - acc: 0.9567 - val_loss: 0.1332 - val_acc: 0.9497\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1099 - acc: 0.9567 - val_loss: 0.1295 - val_acc: 0.9520\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1093 - acc: 0.9560 - val_loss: 0.1238 - val_acc: 0.9563\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1093 - acc: 0.9572 - val_loss: 0.1281 - val_acc: 0.9543\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1100 - acc: 0.9568 - val_loss: 0.1260 - val_acc: 0.9533\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1118 - acc: 0.9565 - val_loss: 0.1235 - val_acc: 0.9533\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1077 - acc: 0.9568 - val_loss: 0.1228 - val_acc: 0.9543\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1081 - acc: 0.9566 - val_loss: 0.1209 - val_acc: 0.9547\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1079 - acc: 0.9578 - val_loss: 0.1247 - val_acc: 0.9530\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1032 - acc: 0.9593 - val_loss: 0.1227 - val_acc: 0.9560\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0998 - acc: 0.9607 - val_loss: 0.1225 - val_acc: 0.9543\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1032 - acc: 0.9593 - val_loss: 0.1237 - val_acc: 0.9557\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1018 - acc: 0.9592 - val_loss: 0.1237 - val_acc: 0.9553\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1028 - acc: 0.9583 - val_loss: 0.1224 - val_acc: 0.9530\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1033 - acc: 0.9593 - val_loss: 0.1195 - val_acc: 0.9580\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1003 - acc: 0.9596 - val_loss: 0.1202 - val_acc: 0.9550\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1007 - acc: 0.9600 - val_loss: 0.1213 - val_acc: 0.9530\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1011 - acc: 0.9593 - val_loss: 0.1230 - val_acc: 0.9547\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1000 - acc: 0.9595 - val_loss: 0.1215 - val_acc: 0.9550\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1032 - acc: 0.9591 - val_loss: 0.1202 - val_acc: 0.9557\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1026 - acc: 0.9586 - val_loss: 0.1206 - val_acc: 0.9550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f746eee9a58>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = batch_gen(train_df)\n",
    "model.fit_generator(mg, epochs=30,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=(val_vects, val_y),\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "689b1f0d231570a429784cf9914a935ba65491b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [00:17, 12.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# prediction part\n",
    "batch_size = 256\n",
    "def batch_gen(test_df):\n",
    "    n_batches = math.ceil(len(test_df) / batch_size)\n",
    "    for i in range(n_batches):\n",
    "        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "        text_arr = np.array([text_to_array(text) for text in texts])\n",
    "        yield text_arr\n",
    "\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "\n",
    "all_preds = []\n",
    "for x in tqdm(batch_gen(test_df)):\n",
    "    all_preds.extend(model.predict(x).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "51134b85fc772ca8732ab671884b20d8062dc155"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51495</th>\n",
       "      <td>e971bace7107d3da76d2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31865</th>\n",
       "      <td>8fc9588e0ac80af5c294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        qid  prediction\n",
       "51495  e971bace7107d3da76d2           0\n",
       "31865  8fc9588e0ac80af5c294           0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te = (np.array(all_preds) > 0.5).astype(np.int)\n",
    "\n",
    "submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\n",
    "submit_df.sample(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
